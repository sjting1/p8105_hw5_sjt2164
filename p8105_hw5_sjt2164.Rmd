---
title: "Homework 5 "
output: github_document
author: Serena T.
---
```{r, include = FALSE}
library(tidyverse)
library(dplyr)
```

## Problem 1

```{r bday function}
#given: no leap years (only 365 days in a yr), bdays are uniformly distributed over the year. 

#sample size of n
bday_dups = function(n) {
  
  bday = sample(1:365, n, replace = TRUE)  #draws bdays per person for size n
  dups = any(duplicated(bday)) #checks dups
  return(dups) #"True" means there are duplicates
}
```

```{r sim}
simulation = 
  expand_grid(
    n = 2:50,
    iter = 1:10000
  ) |>
  mutate(
    estimate = map_lgl(n, bday_dups)) |>
       group_by(n) |> 
    summarize(p = mean(estimate)) #averaging across the sim runs
```

```{r sim plot}
sim_plot = simulation |> 
  ggplot(aes(x = n, y = p)) + 
  geom_point() + 
  geom_line() +
  labs(
    title = "Probability of Same Birthdays Across Group Size",
    x = "Group Size (n)", 
    y = "Probability") +
   scale_x_continuous(
      limits = c(0, 50),
    breaks = seq(0,50, by = 5))

sim_plot
```

The graph shows an S curve, which indicates that as group size increases past n = 50, the curve would reach a plateau. This makes sense since probability ranges from 0 to 1.
As the group size increases, the probability that at least 2 people will share a birthday increases towards 1.00. Around a group size of 23, the probability is 50%.

## Problem 2: One-sample T-test

```{r}
# set design elements: n=30, alpha = 5, mu = 0

sample_size = 30
sigma_std = 5

mu = 0
alpha = 0.05

#function
Q2_function = function(n = sample_size, mean = mu, sd = sigma_std) {
  
  sim_df =  tibble(x = rnorm(n, mean, sd))
  
  ttest = broom::tidy(t.test(sim_df, mu = 0)) #broom::tidy to clean output
}

# tested by running sim_test(30, 0, 5). Output columns = estimate, statistic, p.value, parameter, conf.low conf.high, method, alternative

#5000 sims and save results from model x~Normal[mu, sigma]
simu = 
  expand_grid(dateset_number = 1:5000) |>
  mutate(
    simu_result = map(dateset_number, ~Q2_function(sample_size, mean = 0, sigma_std))) |>
  unnest(simu_result) |>
  janitor::clean_names()

#save mu_hat and p-value for each dataset
saved_values = simu |>
  select(estimate, p_value) #estimate = mu hat

#repeat for mu = {1,2,3,4,5,6}
simu_mu = 
  expand_grid(
    dateset_number = 1:5000,
    mus = c(1,2,3,4,5,6)) |>
  mutate(
    simu_result_mu = map(mus, ~Q2_function(sample_size, mean = mus, sigma_std))) |>
  unnest(simu_result_mu) |>
  janitor::clean_names()

saved_values_mu = simu_mu |>
  select(mus, estimate, p_value)
```

```{r plot1}
#make plot: y = proportion of times null was rejected (power); x = true value of mu
#reject null if p-value < 0.05
#bind rows since same variables "estimate" "p_value"

combined_data = bind_rows(saved_values, saved_values_mu) #35k observations 

# summarize proportion
#mus of NA = 0

#y = proportion of times the null was rejected (the power of the test)
#x = true value of 𝜇

```

The association between effect size and power as shown in the graph above suggests that....

```{r plot2}
#make plot A: y = average estimate of mu_hat; x = true value of mu
#plot B: y=average estimate of mu_hat only in samples for which null was rejected; x = true value of mu 
```

Q: Is the sample average of mu hat across tests for which the null is rejected approximlately equal to true value of mu? Why or why not?

## Problem 3: Homicides in 50 Large U.S. cities

```{r import data, include = FALSE}
homicide = read_csv("./data/homicide-data.csv", na = c("NA", ".", "")) |>
  janitor::clean_names()

#52,179 observations and 12 variables
#victim_age has numeric values and some "unknown"

summary(homicide)
#char = uid, victim_last, victim_first, victim_race, victim_age, victim_sex, city, state, and disposition
#dbl = reported_date, lat, lon

homicide |> distinct(victim_race) #Hispanic, White, Other, Black, Asian, and Unknown
homicide |> distinct(victim_sex) #male, female, unknown
homicide |> distinct(victim_age) #102 distinct ages, including unknown 
homicide |> distinct(city) #50
homicide |> distinct(state) #28 states in abbrev. (wisconsin = "wI"?)
homicide |> distinct(disposition) #3 categories: closed without arrest, closed by arrest, open/no arrest

#sorting reported_date: 2 dates have an extra number (201511105, 201511018)
```

**Raw Data**
The raw data `homicide` has `r nrow(homicide)` observations and `r ncol(homicide)` variables.

* Identity variables include 
  * `uid` (`r count(distinct(homicide, uid))`), 
  * Name of victim (`victim_first` and `victim_last`)
  * `victim_race` (Hispanic, White, Other, Black, Asian, and Unknown), 
  * `victim_age` (includes Unknown)
  * `victim_sex` (Male, Female, Unknown)
* Date variable includes `reported_date` in the format YYYYMMDD. 2 entries does not follow this format (has an extra number).
* Location variables include `city` (50), `state`, latitude (`lat`), and longitude (`lon`). 
* The last variable is `disposition` (`r count(distinct(homicide, disposition))` categories)

**Data Cleaning**

* Fixed two `reported_date` entries that had an extra number
* Fixed abbreviation for Wisconsin
* created new `city_state` variable

```{r fix}
homicide_data = homicide |>
  mutate(
    reported_date = case_match(reported_date,
      201511105 ~ 20151105,
      201511018 ~ 20151018,
      .default = reported_date),
    reported_date = as.Date.character(reported_date, format = "%Y%m%d"),
     state = case_match(state,
      'wI' ~ 'WI',
      .default = state),
    victim_race = as.factor(victim_race),
    victim_sex = as.factor(victim_sex),
    city = as.factor(city),
    state = as.factor(state),
    disposition = as.factor(disposition)) |>
  mutate(city_state = paste(city, state, sep = ", ")) #create city_state var
  
#age as a character with "unknown" & numeric values. Keep/use factor?
  
#Formatted date and checked so that:  
#GERALD A. BUNCH: 201511105 -> 20151105 -> 2015-11-05 
#LUIS SALAS: 201511018 -> 20151018 ->2015-10-18
#wisconsin = WI
```

**Table 1: Total vs. Unsolved Homicides**

```{r summary}

#summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”)

summary = homicide_data |>
  group_by(city_state) |>
  summarize(
    homicide_total = n(),
    homicide_unsolv = sum(disposition %in% c("Closed without arrest", "Open/No arrest")))


knitr::kable(summary, col.names = c("City, State", "Total Homicides", "Unsolved Homicides")) 
```
The table above compares the total number of homicides to the number of unsolved homicides in each of the 50 locations by city, state. 

**Table 2: Unsolved Homicides in Baltimore, MD**
```{r baltimore}
#use the prop.test and use broom::tidy 
#pull the estimated proportion and confidence intervals

Baltimore_MD = summary |>
  filter(city_state == "Baltimore, MD") |>
  summarize(
    test = list(prop.test(homicide_unsolv, homicide_total))) |>
    mutate(result = map(test, broom::tidy)) |>
   unnest(result) |>
  janitor::clean_names() |>
  select(estimate,conf_low, conf_high)

knitr::kable(Baltimore_MD, digits = 2, col.names = c("Estimated Proportion of Unsolved Homicides", "Lower Limit of 95% CI", "Upper Limit of 95% CI")) 
```

The table above shows the estimated proportion of Unsolved Homicides in Baltimore, Maryland along with the 95% confidence interval. 

65% of the total homicides in Baltimore, MD are unsolved. We are 95% confident that the true proportion of unsolved homicides in Baltimore, MD is between 63% and 66%.

**Table 3: Unsolved Homicides in All 50 Locations**

```{r all location}
#Now run prop.test for each of the cities in your dataset
#extract both the proportion of unsolved homicides 95%CI for each. 
#Do this within a “tidy” pipeline, making use of purrr::map, purrr::map2, list columns and unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city.

All_cities = summary |>
   mutate(
    test_all = map2(homicide_unsolv, homicide_total, prop.test),
    result_all = map(test_all, broom::tidy)) |>
    unnest(result_all) |>
  janitor::clean_names() |> 
  select(city_state, estimate,conf_low, conf_high) |>
  arrange(city_state)

knitr::kable(All_cities, digits = 2, col.names = c("Location (City, State)", "Estimated Proportion of Unsolved Homicides", "Lower Limit of 95% CI", "Upper Limit of 95% CI")) 
```

The table above is ordered by city name and shows the estimated proportion of unsolved homicides in all 50 locations listed in the dataset along with the 95% confidence interval.

**Plot**

```{r estimate plot}
plot = All_cities |>
  ggplot(aes(x = reorder(city_state, estimate), y = estimate)) + #order cities by proportion
  geom_point() +
  geom_errorbar(aes(ymin = conf_low, ymax = conf_high)) + #add error bars
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  labs(title = "Unsolved Homicides Across 50 Cities",
       x = "Location (City, State)",
       y = "Estimated Proportion of Unsolved Homicides")

plot
```

The plot above shows the distribution of the estimated proportion of unsolved homicides across 50 different cities. Chicago, IL has the highest proportion with 74% of total homicides being unsolved homicides (95%CI: 0.72, 0.75). On the other hand, Tulsa, AL has the lowest with 0% (95%CI: 0.00,0.95). A majority of the cities had an estimated proportion of unsolved homicides under 50%. 
