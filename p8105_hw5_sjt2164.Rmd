---
title: "Homework 5 "
output: github_document
---
```{r, include = FALSE}
library(tidyverse)
library(dplyr)
```

## Problem 1

```{r bday function}
#given: no leap years (only 365 days in a yr), bdays are uniformly distributed over the year. 

#sample size of n
bday_dups = function(n) {
  
  bday = sample(1:365, n, replace = TRUE)  #draws bdays per person for size n
  dups = any(duplicated(bday)) #checks dups
  return(dups) #"True" means there are duplicates
}
```

```{r sim}
simulation = 
  expand_grid(
    n = 2:50,
    iter = 1:10000
  ) |>
  mutate(
    estimate = map_lgl(n, bday_dups)) |>
       group_by(n) |> 
    summarize(p = mean(estimate)) #averaging across the sim runs
```

```{r sim plot}
sim_plot = simulation |> 
  ggplot(aes(x = n, y = p)) + 
  geom_point() + 
  geom_line() +
  labs(
    title = "Probability of Same Birthdays Across Group Size",
    x = "Group Size (n)", 
    y = "Probability") +
   scale_x_continuous(
      limits = c(0, 50),
    breaks = seq(0,50, by = 5))

sim_plot
```

The graph shows an S curve, which indicates that as group size increases past n = 50, the curve would reach a plateau. This makes sense since probability ranges from 0 to 1.
As the group size increases, the probability that at least 2 people will share a birthday increases towards 1.00. Around a group size of 23, the probability is 50%.

## Problem 2

```{r}
# set design elements: n=30, alpha = 5, mu = 0

sample_size = 30
sigma_std = 5

mu = 0
alpha = 0.05

#function
Q2_function = function(n, mean, sd) {
  
  dataset = rnorm(n, mean, sd)
  
  ttest = broom::tidy(t.test(dataset, mu = 0)) 
  
  return(data.frame(
    sample_mean = mean(dataset),
    p_value = ttest$p.value
  ))
}

#5000 sims and save results

# generate 5000 datasets from model x~Normal[mu, sigma]

#save mu_hat and p-value for each dataset

#Use broom::tidy to clean output of t.test()
#repeat for mu = {1,2,3,4,5,6}
```

```{r plot1}
#make plot: y = proportion of times null was rejected (power); x = true value of mu
```

The association between effect size and power as shown in the graph above suggests that....

```{r plot2}
#make plot A: y = average estimate of mu_hat; x = true value of mu
#plot B: y=average estimate of mu_hat only in samples for which null was rejected; x = true value of mu 
```

Q: Is the sample average of mu hat across tests for which the null is rejected approximlately equal to true value of mu? Why or why not?

## Problem 3: Homicides in 50 Large U.S. cities

```{r import data, include = FALSE}
homicide = read_csv("./data/homicide-data.csv", na = c("NA", ".", "")) |>
  janitor::clean_names()

#52,179 observations and 12 variables
#victim_age has numeric values and some "unknown"

summary(homicide)
#char = uid, victim_last, victim_first, victim_race, victim_age, victim_sex, city, state, and disposition
#dbl = reported_date, lat, lon

homicide |> distinct(victim_race) #Hispanic, White, Other, Black, Asian, and Unknown
homicide |> distinct(victim_sex) #male, female, unknown
homicide |> distinct(victim_age) #102 distinct ages, including unknown 
homicide |> distinct(city) #50
homicide |> distinct(state) #28 states in abbrev. (wisconsin = "wI"?)
homicide |> distinct(disposition) #3 categories: closed without arrest, closed by arrest, open/no arrest

#sorting reported_date: 2 dates have an extra number (201511105, 201511018)
```

**Raw Data**
The raw data `homicide` has `r nrow(homicide)` observations and `r ncol(homicide)` variables. 

* Identity variables include 
  * `uid` (`r count(distinct(homicide, uid))`), 
  * Name of victim (`victim_first` and `victim_last`)
  * `victim_race` (Hispanic, White, Other, Black, Asian, and Unknown), 
  * `victim_age` (includes Unknown)
  * `victim_sex` (Male, Female, Unknown)
* Date variable includes `reported_date` in the format YYYYMMDD. 2 entries does not follow this format (has an extra number).
* Location variables include `city` (50), `state`, latitude (`lat`), and longitude (`lon`). 
* The last variable is `disposition` (`r count(distinct(homicide, disposition))` categories)

```{r fix}
homicide_data = homicide |>
  mutate(
    reported_date = case_match(reported_date,
      201511105 ~ 20151105,
      201511018 ~ 20151018,
      .default = reported_date),
    reported_date = as.Date.character(reported_date, format = "%Y%m%d"),
     state = case_match(state,
      'wI' ~ 'WI',
      .default = state),
    victim_race = as.factor(victim_race),
    victim_sex = as.factor(victim_sex),
    city = as.factor(city),
    state = as.factor(state),
    disposition = as.factor(disposition)) |>
  mutate(city_state = paste(city, state, sep = ", ")) #create city_state var
  
#age as a character with "unknown" & numeric values. Keep/use factor?
  
#Formatted date and checked so that:  
#GERALD A. BUNCH: 201511105 -> 20151105 -> 2015-11-05 
#LUIS SALAS: 201511018 -> 20151018 ->2015-10-18
#wisconsin = WI
```

```{r summary}

#summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”)

summary = homicide_data |>
  group_by(city_state) |>
  summarize(
    homicide_total = n(),
    homicide_unsolv = sum(disposition %in% c("Closed without arrest", "Open/No arrest")))

```

```{r}
#For the city of Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object, apply the broom::tidy to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.
```

```{r}
#Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. Do this within a “tidy” pipeline, making use of purrr::map, purrr::map2, list columns and unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city.
```

**Plot**

```{r}
#Showing estimates and CIs for each city
#geom_errorbar to add error bars based on the upper and lower limits. 
#Organize cities according to the proportion of unsolved homicides.


```

