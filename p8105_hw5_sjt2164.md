Homework 5
================

## Problem 1

``` r
#given: no leap years (only 365 days in a yr), bdays are uniformly distributed over the year. 

#sample size of n
bday_dups = function(n) {
  
  bday = sample(1:365, n, replace = TRUE)  #draws bdays per person for size n
  dups = any(duplicated(bday)) #checks dups
  return(dups) #"True" means there are duplicates
}
```

``` r
simulation = 
  expand_grid(
    n = 2:50,
    iter = 1:10000
  ) |>
  mutate(
    estimate = map_lgl(n, bday_dups)) |>
       group_by(n) |> 
    summarize(p = mean(estimate)) #averaging across the sim runs
```

``` r
sim_plot = simulation |> 
  ggplot(aes(x = n, y = p)) + 
  geom_point() + 
  geom_line() +
  labs(
    title = "Probability of Same Birthdays Across Group Size",
    x = "Group Size (n)", 
    y = "Probability") +
   scale_x_continuous(
      limits = c(0, 50),
    breaks = seq(0,50, by = 5))

sim_plot
```

![](p8105_hw5_sjt2164_files/figure-gfm/sim%20plot-1.png)<!-- -->

The graph shows an S curve, which indicates that as group size increases
past n = 50, the curve would reach a plateau. This makes sense since
probability ranges from 0 to 1. As the group size increases, the
probability that at least 2 people will share a birthday increases
towards 1.00. Around a group size of 23, the probability is 50%.

## Problem 2

``` r
# set design elements: n=30, alpha = 5, mu = 0

sample_size = 30
sigma_std = 5

mu = 0
alpha = 0.05

#function
Q2_function = function(n, mean, sd) {
  
  dataset = rnorm(n, mean, sd)
  
  ttest = broom::tidy(t.test(dataset, mu = 0)) 
  
  return(data.frame(
    sample_mean = mean(dataset),
    p_value = ttest$p.value
  ))
}

#5000 sims and save results

# generate 5000 datasets from model x~Normal[mu, sigma]

#save mu_hat and p-value for each dataset

#Use broom::tidy to clean output of t.test()
#repeat for mu = {1,2,3,4,5,6}
```

``` r
#make plot: y = proportion of times null was rejected (power); x = true value of mu
```

The association between effect size and power as shown in the graph
above suggests that….

``` r
#make plot A: y = average estimate of mu_hat; x = true value of mu
#plot B: y=average estimate of mu_hat only in samples for which null was rejected; x = true value of mu 
```

Q: Is the sample average of mu hat across tests for which the null is
rejected approximlately equal to true value of mu? Why or why not?

## Problem 3: Homicides in 50 Large U.S. cities

**Raw Data** The raw data `homicide` has 52179 observations and 12
variables.

- Identity variables include
  - `uid` (52179),
  - Name of victim (`victim_first` and `victim_last`)
  - `victim_race` (Hispanic, White, Other, Black, Asian, and Unknown),
  - `victim_age` (includes Unknown)
  - `victim_sex` (Male, Female, Unknown)
- Date variable includes `reported_date` in the format YYYYMMDD. 2
  entries does not follow this format (has an extra number).
- Location variables include `city` (50), `state`, latitude (`lat`), and
  longitude (`lon`).
- The last variable is `disposition` (3 categories)

**Data Cleaning**

- Fixed two `reported_date` entries that had an extra number
- Fixed abbreviation for Wisconsin
- created new `city_state` variable

``` r
homicide_data = homicide |>
  mutate(
    reported_date = case_match(reported_date,
      201511105 ~ 20151105,
      201511018 ~ 20151018,
      .default = reported_date),
    reported_date = as.Date.character(reported_date, format = "%Y%m%d"),
     state = case_match(state,
      'wI' ~ 'WI',
      .default = state),
    victim_race = as.factor(victim_race),
    victim_sex = as.factor(victim_sex),
    city = as.factor(city),
    state = as.factor(state),
    disposition = as.factor(disposition)) |>
  mutate(city_state = paste(city, state, sep = ", ")) #create city_state var
  
#age as a character with "unknown" & numeric values. Keep/use factor?
  
#Formatted date and checked so that:  
#GERALD A. BUNCH: 201511105 -> 20151105 -> 2015-11-05 
#LUIS SALAS: 201511018 -> 20151018 ->2015-10-18
#wisconsin = WI
```

**Homicide: Total vs. Unsolved**

``` r
#summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”)

summary = homicide_data |>
  group_by(city_state) |>
  summarize(
    homicide_total = n(),
    homicide_unsolv = sum(disposition %in% c("Closed without arrest", "Open/No arrest")))


knitr::kable(summary, col.names = c("City, State", "Total Homicides", "Unsolved Homicides")) 
```

| City, State        | Total Homicides | Unsolved Homicides |
|:-------------------|----------------:|-------------------:|
| Albuquerque, NM    |             378 |                146 |
| Atlanta, GA        |             973 |                373 |
| Baltimore, MD      |            2827 |               1825 |
| Baton Rouge, LA    |             424 |                196 |
| Birmingham, AL     |             800 |                347 |
| Boston, MA         |             614 |                310 |
| Buffalo, NY        |             521 |                319 |
| Charlotte, NC      |             687 |                206 |
| Chicago, IL        |            5535 |               4073 |
| Cincinnati, OH     |             694 |                309 |
| Columbus, OH       |            1084 |                575 |
| Dallas, TX         |            1567 |                754 |
| Denver, CO         |             312 |                169 |
| Detroit, MI        |            2519 |               1482 |
| Durham, NC         |             276 |                101 |
| Fort Worth, TX     |             549 |                255 |
| Fresno, CA         |             487 |                169 |
| Houston, TX        |            2942 |               1493 |
| Indianapolis, IN   |            1322 |                594 |
| Jacksonville, FL   |            1168 |                597 |
| Kansas City, MO    |            1190 |                486 |
| Las Vegas, NV      |            1381 |                572 |
| Long Beach, CA     |             378 |                156 |
| Los Angeles, CA    |            2257 |               1106 |
| Louisville, KY     |             576 |                261 |
| Memphis, TN        |            1514 |                483 |
| Miami, FL          |             744 |                450 |
| Milwaukee, WI      |            1115 |                403 |
| Minneapolis, MN    |             366 |                187 |
| Nashville, TN      |             767 |                278 |
| New Orleans, LA    |            1434 |                930 |
| New York, NY       |             627 |                243 |
| Oakland, CA        |             947 |                508 |
| Oklahoma City, OK  |             672 |                326 |
| Omaha, NE          |             409 |                169 |
| Philadelphia, PA   |            3037 |               1360 |
| Phoenix, AZ        |             914 |                504 |
| Pittsburgh, PA     |             631 |                337 |
| Richmond, VA       |             429 |                113 |
| Sacramento, CA     |             376 |                139 |
| San Antonio, TX    |             833 |                357 |
| San Bernardino, CA |             275 |                170 |
| San Diego, CA      |             461 |                175 |
| San Francisco, CA  |             663 |                336 |
| Savannah, GA       |             246 |                115 |
| St. Louis, MO      |            1677 |                905 |
| Stockton, CA       |             444 |                266 |
| Tampa, FL          |             208 |                 95 |
| Tulsa, AL          |               1 |                  0 |
| Tulsa, OK          |             583 |                193 |
| Washington, DC     |            1345 |                589 |

The table above compares the total number of homicides to the number of
unsolved homicides in each of the 50 locations by city, state.

**Unsolved Homicides in Baltimore, MD**

``` r
#use the prop.test and use broom::tidy 
#pull the estimated proportion and confidence intervals

Baltimore_MD = summary |>
  filter(city_state == "Baltimore, MD") |>
  summarize(
    test = list(prop.test(homicide_unsolv, homicide_total))) |>
    mutate(result = map(test, broom::tidy)) |>
   unnest(result) |>
  janitor::clean_names() |>
  select(estimate,conf_low, conf_high)

knitr::kable(Baltimore_MD, digits = 2, col.names = c("Estimated Proportion of Unsolved Homicides", "Lower Limit of 95% CI", "Upper Limit of 95% CI")) 
```

| Estimated Proportion of Unsolved Homicides | Lower Limit of 95% CI | Upper Limit of 95% CI |
|---:|---:|---:|
| 0.65 | 0.63 | 0.66 |

The table above shows the estimated proportion of Unsolved Homicides in
Baltimore, Maryland along with the 95% confidence interval.

``` r
#Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. Do this within a “tidy” pipeline, making use of purrr::map, purrr::map2, list columns and unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city.
```

**Plot**

``` r
#Showing estimates and CIs for each city
#geom_errorbar to add error bars based on the upper and lower limits. 
#Organize cities according to the proportion of unsolved homicides.
```
