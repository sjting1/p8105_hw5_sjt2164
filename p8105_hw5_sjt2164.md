Homework 5
================
Serena T.

## Problem 1

``` r
#given: no leap years (only 365 days in a yr), bdays are uniformly distributed over the year. 

#sample size of n
bday_dups = function(n) {
  
  bday = sample(1:365, n, replace = TRUE)  #draws bdays per person for size n
  dups = any(duplicated(bday)) #checks dups
  return(dups) #"True" means there are duplicates
}
```

``` r
simulation = 
  expand_grid(
    n = 2:50,
    iter = 1:10000
  ) |>
  mutate(
    estimate = map_lgl(n, bday_dups)) |>
       group_by(n) |> 
    summarize(p = mean(estimate)) #averaging across the sim runs
```

``` r
sim_plot = simulation |> 
  ggplot(aes(x = n, y = p)) + 
  geom_point() + 
  geom_line() +
  labs(
    title = "Probability of Same Birthdays Across Group Size",
    x = "Group Size (n)", 
    y = "Probability") +
   scale_x_continuous(
      limits = c(0, 50),
    breaks = seq(0,50, by = 5))

sim_plot
```

![](p8105_hw5_sjt2164_files/figure-gfm/sim%20plot-1.png)<!-- -->

The graph shows an S curve, which indicates that as group size increases
past n = 50, the curve would reach a plateau. This makes sense since
probability ranges from 0 to 1. As the group size increases, the
probability that at least 2 people will share a birthday increases
towards 1.00. Around a group size of 23, the probability is 50%.

## Problem 2

``` r
# set design elements: n=30, alpha = 5, mu = 0

sample_size = 30
sigma_std = 5

mu = 0
alpha = 0.05

#function
Q2_function = function(n, mean, sd) {
  
  dataset = rnorm(n, mean, sd)
  
  ttest = broom::tidy(t.test(dataset, mu = 0)) 
  
  return(data.frame(
    sample_mean = mean(dataset),
    p_value = ttest$p.value
  ))
}

#5000 sims and save results

# generate 5000 datasets from model x~Normal[mu, sigma]

#save mu_hat and p-value for each dataset

#Use broom::tidy to clean output of t.test()
#repeat for mu = {1,2,3,4,5,6}
```

``` r
#make plot: y = proportion of times null was rejected (power); x = true value of mu
```

The association between effect size and power as shown in the graph
above suggests that….

``` r
#make plot A: y = average estimate of mu_hat; x = true value of mu
#plot B: y=average estimate of mu_hat only in samples for which null was rejected; x = true value of mu 
```

Q: Is the sample average of mu hat across tests for which the null is
rejected approximlately equal to true value of mu? Why or why not?

## Problem 3: Homicides in 50 Large U.S. cities

**Raw Data** The raw data `homicide` has 52179 observations and 12
variables.

- Identity variables include
  - `uid` (52179),
  - Name of victim (`victim_first` and `victim_last`)
  - `victim_race` (Hispanic, White, Other, Black, Asian, and Unknown),
  - `victim_age` (includes Unknown)
  - `victim_sex` (Male, Female, Unknown)
- Date variable includes `reported_date` in the format YYYYMMDD. 2
  entries does not follow this format (has an extra number).
- Location variables include `city` (50), `state`, latitude (`lat`), and
  longitude (`lon`).
- The last variable is `disposition` (3 categories)

**Data Cleaning**

- Fixed two `reported_date` entries that had an extra number
- Fixed abbreviation for Wisconsin
- created new `city_state` variable

``` r
homicide_data = homicide |>
  mutate(
    reported_date = case_match(reported_date,
      201511105 ~ 20151105,
      201511018 ~ 20151018,
      .default = reported_date),
    reported_date = as.Date.character(reported_date, format = "%Y%m%d"),
     state = case_match(state,
      'wI' ~ 'WI',
      .default = state),
    victim_race = as.factor(victim_race),
    victim_sex = as.factor(victim_sex),
    city = as.factor(city),
    state = as.factor(state),
    disposition = as.factor(disposition)) |>
  mutate(city_state = paste(city, state, sep = ", ")) #create city_state var
  
#age as a character with "unknown" & numeric values. Keep/use factor?
  
#Formatted date and checked so that:  
#GERALD A. BUNCH: 201511105 -> 20151105 -> 2015-11-05 
#LUIS SALAS: 201511018 -> 20151018 ->2015-10-18
#wisconsin = WI
```

**Table 1: Total vs. Unsolved Homicides**

``` r
#summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”)

summary = homicide_data |>
  group_by(city_state) |>
  summarize(
    homicide_total = n(),
    homicide_unsolv = sum(disposition %in% c("Closed without arrest", "Open/No arrest")))


knitr::kable(summary, col.names = c("City, State", "Total Homicides", "Unsolved Homicides")) 
```

| City, State        | Total Homicides | Unsolved Homicides |
|:-------------------|----------------:|-------------------:|
| Albuquerque, NM    |             378 |                146 |
| Atlanta, GA        |             973 |                373 |
| Baltimore, MD      |            2827 |               1825 |
| Baton Rouge, LA    |             424 |                196 |
| Birmingham, AL     |             800 |                347 |
| Boston, MA         |             614 |                310 |
| Buffalo, NY        |             521 |                319 |
| Charlotte, NC      |             687 |                206 |
| Chicago, IL        |            5535 |               4073 |
| Cincinnati, OH     |             694 |                309 |
| Columbus, OH       |            1084 |                575 |
| Dallas, TX         |            1567 |                754 |
| Denver, CO         |             312 |                169 |
| Detroit, MI        |            2519 |               1482 |
| Durham, NC         |             276 |                101 |
| Fort Worth, TX     |             549 |                255 |
| Fresno, CA         |             487 |                169 |
| Houston, TX        |            2942 |               1493 |
| Indianapolis, IN   |            1322 |                594 |
| Jacksonville, FL   |            1168 |                597 |
| Kansas City, MO    |            1190 |                486 |
| Las Vegas, NV      |            1381 |                572 |
| Long Beach, CA     |             378 |                156 |
| Los Angeles, CA    |            2257 |               1106 |
| Louisville, KY     |             576 |                261 |
| Memphis, TN        |            1514 |                483 |
| Miami, FL          |             744 |                450 |
| Milwaukee, WI      |            1115 |                403 |
| Minneapolis, MN    |             366 |                187 |
| Nashville, TN      |             767 |                278 |
| New Orleans, LA    |            1434 |                930 |
| New York, NY       |             627 |                243 |
| Oakland, CA        |             947 |                508 |
| Oklahoma City, OK  |             672 |                326 |
| Omaha, NE          |             409 |                169 |
| Philadelphia, PA   |            3037 |               1360 |
| Phoenix, AZ        |             914 |                504 |
| Pittsburgh, PA     |             631 |                337 |
| Richmond, VA       |             429 |                113 |
| Sacramento, CA     |             376 |                139 |
| San Antonio, TX    |             833 |                357 |
| San Bernardino, CA |             275 |                170 |
| San Diego, CA      |             461 |                175 |
| San Francisco, CA  |             663 |                336 |
| Savannah, GA       |             246 |                115 |
| St. Louis, MO      |            1677 |                905 |
| Stockton, CA       |             444 |                266 |
| Tampa, FL          |             208 |                 95 |
| Tulsa, AL          |               1 |                  0 |
| Tulsa, OK          |             583 |                193 |
| Washington, DC     |            1345 |                589 |

The table above compares the total number of homicides to the number of
unsolved homicides in each of the 50 locations by city, state.

**Table 2: Unsolved Homicides in Baltimore, MD**

``` r
#use the prop.test and use broom::tidy 
#pull the estimated proportion and confidence intervals

Baltimore_MD = summary |>
  filter(city_state == "Baltimore, MD") |>
  summarize(
    test = list(prop.test(homicide_unsolv, homicide_total))) |>
    mutate(result = map(test, broom::tidy)) |>
   unnest(result) |>
  janitor::clean_names() |>
  select(estimate,conf_low, conf_high)

knitr::kable(Baltimore_MD, digits = 2, col.names = c("Estimated Proportion of Unsolved Homicides", "Lower Limit of 95% CI", "Upper Limit of 95% CI")) 
```

| Estimated Proportion of Unsolved Homicides | Lower Limit of 95% CI | Upper Limit of 95% CI |
|---:|---:|---:|
| 0.65 | 0.63 | 0.66 |

The table above shows the estimated proportion of Unsolved Homicides in
Baltimore, Maryland along with the 95% confidence interval.

65% of the total homicides in Baltimore, MD are unsolved. We are 95%
confident that the true proportion of unsolved homicides in Baltimore,
MD is between 63% and 66%.

**Table 3: Unsolved Homicides in All 50 Locations**

``` r
#Now run prop.test for each of the cities in your dataset
#extract both the proportion of unsolved homicides 95%CI for each. 
#Do this within a “tidy” pipeline, making use of purrr::map, purrr::map2, list columns and unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city.

All_cities = summary |>
   mutate(
    test_all = map2(homicide_unsolv, homicide_total, prop.test),
    result_all = map(test_all, broom::tidy)) |>
    unnest(result_all) |>
  janitor::clean_names() |> 
  select(city_state, estimate,conf_low, conf_high) |>
  arrange(city_state)
```

    ## Warning: There was 1 warning in `mutate()`.
    ## ℹ In argument: `test_all = map2(homicide_unsolv, homicide_total, prop.test)`.
    ## Caused by warning in `.f()`:
    ## ! Chi-squared approximation may be incorrect

``` r
knitr::kable(All_cities, digits = 2, col.names = c("Location (City, State)", "Estimated Proportion of Unsolved Homicides", "Lower Limit of 95% CI", "Upper Limit of 95% CI")) 
```

| Location (City, State) | Estimated Proportion of Unsolved Homicides | Lower Limit of 95% CI | Upper Limit of 95% CI |
|:---|---:|---:|---:|
| Albuquerque, NM | 0.39 | 0.34 | 0.44 |
| Atlanta, GA | 0.38 | 0.35 | 0.41 |
| Baltimore, MD | 0.65 | 0.63 | 0.66 |
| Baton Rouge, LA | 0.46 | 0.41 | 0.51 |
| Birmingham, AL | 0.43 | 0.40 | 0.47 |
| Boston, MA | 0.50 | 0.46 | 0.55 |
| Buffalo, NY | 0.61 | 0.57 | 0.65 |
| Charlotte, NC | 0.30 | 0.27 | 0.34 |
| Chicago, IL | 0.74 | 0.72 | 0.75 |
| Cincinnati, OH | 0.45 | 0.41 | 0.48 |
| Columbus, OH | 0.53 | 0.50 | 0.56 |
| Dallas, TX | 0.48 | 0.46 | 0.51 |
| Denver, CO | 0.54 | 0.48 | 0.60 |
| Detroit, MI | 0.59 | 0.57 | 0.61 |
| Durham, NC | 0.37 | 0.31 | 0.43 |
| Fort Worth, TX | 0.46 | 0.42 | 0.51 |
| Fresno, CA | 0.35 | 0.31 | 0.39 |
| Houston, TX | 0.51 | 0.49 | 0.53 |
| Indianapolis, IN | 0.45 | 0.42 | 0.48 |
| Jacksonville, FL | 0.51 | 0.48 | 0.54 |
| Kansas City, MO | 0.41 | 0.38 | 0.44 |
| Las Vegas, NV | 0.41 | 0.39 | 0.44 |
| Long Beach, CA | 0.41 | 0.36 | 0.46 |
| Los Angeles, CA | 0.49 | 0.47 | 0.51 |
| Louisville, KY | 0.45 | 0.41 | 0.49 |
| Memphis, TN | 0.32 | 0.30 | 0.34 |
| Miami, FL | 0.60 | 0.57 | 0.64 |
| Milwaukee, WI | 0.36 | 0.33 | 0.39 |
| Minneapolis, MN | 0.51 | 0.46 | 0.56 |
| Nashville, TN | 0.36 | 0.33 | 0.40 |
| New Orleans, LA | 0.65 | 0.62 | 0.67 |
| New York, NY | 0.39 | 0.35 | 0.43 |
| Oakland, CA | 0.54 | 0.50 | 0.57 |
| Oklahoma City, OK | 0.49 | 0.45 | 0.52 |
| Omaha, NE | 0.41 | 0.37 | 0.46 |
| Philadelphia, PA | 0.45 | 0.43 | 0.47 |
| Phoenix, AZ | 0.55 | 0.52 | 0.58 |
| Pittsburgh, PA | 0.53 | 0.49 | 0.57 |
| Richmond, VA | 0.26 | 0.22 | 0.31 |
| Sacramento, CA | 0.37 | 0.32 | 0.42 |
| San Antonio, TX | 0.43 | 0.39 | 0.46 |
| San Bernardino, CA | 0.62 | 0.56 | 0.68 |
| San Diego, CA | 0.38 | 0.34 | 0.43 |
| San Francisco, CA | 0.51 | 0.47 | 0.55 |
| Savannah, GA | 0.47 | 0.40 | 0.53 |
| St. Louis, MO | 0.54 | 0.52 | 0.56 |
| Stockton, CA | 0.60 | 0.55 | 0.64 |
| Tampa, FL | 0.46 | 0.39 | 0.53 |
| Tulsa, AL | 0.00 | 0.00 | 0.95 |
| Tulsa, OK | 0.33 | 0.29 | 0.37 |
| Washington, DC | 0.44 | 0.41 | 0.46 |

The table above is ordered by city name and shows the estimated
proportion of unsolved homicides in all 50 locations listed in the
dataset along with the 95% confidence interval.

**Plot**

``` r
plot = All_cities |>
  ggplot(aes(x = reorder(city_state, estimate), y = estimate)) + #order cities by proportion
  geom_point() +
  geom_errorbar(aes(ymin = conf_low, ymax = conf_high)) + #add error bars
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  labs(title = "Unsolved Homicides Across 50 Cities",
       x = "Location (City, State)",
       y = "Estimated Proportion of Unsolved Homicides")

plot
```

![](p8105_hw5_sjt2164_files/figure-gfm/estimate%20plot-1.png)<!-- -->

The plot above shows the distribution of the estimated proportion of
unsolved homicides across 50 different cities. Chicago, IL has the
highest proportion with 74% of total homicides being unsolved homicides
(95%CI: 0.72, 0.75). On the other hand, Tulsa, AL has the lowest with 0%
(95%CI: 0.00,0.95). A majority of the cities had an estimated proportion
of unsolved homicides under 50%.
